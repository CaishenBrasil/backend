{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About this Project \u00b6 This project goal is simple: Learn REST API development by developing one. This is a personal project that I've came up with to learn a little bit of web development, backend engineering and overall best practices. While developing, there are several doubts that comes up along the way, this documentation aims to address those once solved doubts, so that others that might have questions similar to those I had may benefit from this project to learn as I have learned while building it. My goal with this documentation is to write a blog about web development, and relate to code I've written here so anyone can clone the repo, start the server on their own machine and experiment. Think of it as a hands-on lab of a REST API backend application. Contributions to the project are very welcomed , as long as you want to also write documentation for the code you're pushing, so everyone can follow along. Contribute to the code: Caishen Backend Project Dependencies \u00b6 I'll list each dependency we use in this project, right here in this section. Some dependencies might have an exclusive document used to explain why and how we use it in this project, so stay tuned for updates! Main Dependencies \u00b6 Python - The BEST programming language ever! Poetry - THE python packaging and dependency manager that pip should be This project wouldn't exist if it weren't for FastAPI , a python Web Framework for building APIs. FastAPI is the backbone of the project. As we rely on FastAPI, Pydantic also plays a huge role on the project. By the way, Pydantic might be my favorite python library ever. SQLAlchemy is also another dependency, the go-to python ORM for talking to SQL databases. We're already using SQLAlchemy 2.0 dialect to make things future proof . There are, of course, other dependencies, but they are not as important as those I've just mentioned, nonetheless, I will explain them below: Uvicorn is a python ASGI web server that runs FastAPI code. alembic is a database migration tool to use together with SQLAlchemy. passlib is a password hashing library that we use not only to hash passwords but also to create random words to be used as CSRFTokens. python-jose is an implementation of the JavaScript Object Signing and Encryption (JOSE) technologies, we use it to handle JSON Web Tokens. aioredis is an async Redis client that we use to handle our Redis cache which is used to store tokens and as a queue for logs as well. asyncpg is the PostgreSQL async client we use. structlog is our logging library, we use this to convert our logs to JSON format so we can handle them easier in the ELK stack. orjson is the JSON (de)serializer library we use together with pydantic and structlog to handle JSON (de)serialization. httpx the async http request library that we use to make http requests towards other services from within our application. Development Dependencies \u00b6 Development dependencies are those used only while developing, on this app, we mainly use code quality and documentation dependencies. A huge thanks to pre-commit which makes code quality assurance much easier. With it we can easily handle git hooks, mostly, pre-commit hooks so we can guarantee code quality is maintained across different contributors. If the pre-commit hook fails, code is not even committed. Another incredible dependency is black which can automatically format code for us, so we don't lose time fixing minor formatting issues. Use black and flake8 and have formatting issues just disappear from your code! Last, but not least (at all!) mypy the static type checker that makes hard to catch bugs, easy to solve. A note on type annotations I though static typing python code was a waste of time, however with editor support and mypy, productivity with static typing is just incredible and the kind of bugs that we avoid by just using such a simple tool like that is worth it. Please, use typing in your projects!","title":"About this project"},{"location":"#about-this-project","text":"This project goal is simple: Learn REST API development by developing one. This is a personal project that I've came up with to learn a little bit of web development, backend engineering and overall best practices. While developing, there are several doubts that comes up along the way, this documentation aims to address those once solved doubts, so that others that might have questions similar to those I had may benefit from this project to learn as I have learned while building it. My goal with this documentation is to write a blog about web development, and relate to code I've written here so anyone can clone the repo, start the server on their own machine and experiment. Think of it as a hands-on lab of a REST API backend application. Contributions to the project are very welcomed , as long as you want to also write documentation for the code you're pushing, so everyone can follow along. Contribute to the code: Caishen Backend","title":"About this Project"},{"location":"#project-dependencies","text":"I'll list each dependency we use in this project, right here in this section. Some dependencies might have an exclusive document used to explain why and how we use it in this project, so stay tuned for updates!","title":"Project Dependencies"},{"location":"#main-dependencies","text":"Python - The BEST programming language ever! Poetry - THE python packaging and dependency manager that pip should be This project wouldn't exist if it weren't for FastAPI , a python Web Framework for building APIs. FastAPI is the backbone of the project. As we rely on FastAPI, Pydantic also plays a huge role on the project. By the way, Pydantic might be my favorite python library ever. SQLAlchemy is also another dependency, the go-to python ORM for talking to SQL databases. We're already using SQLAlchemy 2.0 dialect to make things future proof . There are, of course, other dependencies, but they are not as important as those I've just mentioned, nonetheless, I will explain them below: Uvicorn is a python ASGI web server that runs FastAPI code. alembic is a database migration tool to use together with SQLAlchemy. passlib is a password hashing library that we use not only to hash passwords but also to create random words to be used as CSRFTokens. python-jose is an implementation of the JavaScript Object Signing and Encryption (JOSE) technologies, we use it to handle JSON Web Tokens. aioredis is an async Redis client that we use to handle our Redis cache which is used to store tokens and as a queue for logs as well. asyncpg is the PostgreSQL async client we use. structlog is our logging library, we use this to convert our logs to JSON format so we can handle them easier in the ELK stack. orjson is the JSON (de)serializer library we use together with pydantic and structlog to handle JSON (de)serialization. httpx the async http request library that we use to make http requests towards other services from within our application.","title":"Main Dependencies"},{"location":"#development-dependencies","text":"Development dependencies are those used only while developing, on this app, we mainly use code quality and documentation dependencies. A huge thanks to pre-commit which makes code quality assurance much easier. With it we can easily handle git hooks, mostly, pre-commit hooks so we can guarantee code quality is maintained across different contributors. If the pre-commit hook fails, code is not even committed. Another incredible dependency is black which can automatically format code for us, so we don't lose time fixing minor formatting issues. Use black and flake8 and have formatting issues just disappear from your code! Last, but not least (at all!) mypy the static type checker that makes hard to catch bugs, easy to solve. A note on type annotations I though static typing python code was a waste of time, however with editor support and mypy, productivity with static typing is just incredible and the kind of bugs that we avoid by just using such a simple tool like that is worth it. Please, use typing in your projects!","title":"Development Dependencies"},{"location":"deployment/","text":"Deployment \u00b6 Set an .env file Make sure to setup the .env file because this will be necessary to start the application using docker Development environment I highly suggest you use linux to deploy this app, if you are on windows make sure you are developing under WSL2 Environment Variables \u00b6 Before running the app, make sure to create a .env file on project root. All environment variables available are declared on on settings.py . As this project uses pydantic BaseSettings, the .env file will populate the Settings class accordingly. Below is a sample of how your .env file shall look. Caishen Enviroment Variables \u00b6 DEV = False LOG_LEVEL = DEBUG GOOGLE_CLIENT_ID = #replace: google-client-id-here GOOGLE_CLIENT_SECRET = #replace: google-client-secret-here GOOGLE_REDIRECT_URL = http://localhost:8000/api/v1/login/google-login-callback/ GOOGLE_DISCOVERY_URL = https://accounts.google.com/.well-known/openid-configuration FACEBOOK_CLIENT_ID = #replace: facebook-app-id-here FACEBOOK_CLIENT_SECRET = #replace: facebook-app-secret-here FACEBOOK_REDIRECT_URL = https://localhost/api/v1/login/facebook-login-callback FACEBOOK_DISCOVERY_URL = https://www.facebook.com/.well-known/openid-configuration/ FACEBOOK_AUTHORIZATION_ENDPOINT = https://facebook.com/dialog/oauth/ FACEBOOK_TOKEN_ENDPOINT = https://graph.facebook.com/oauth/access_token FACEBOOK_USERINFO_ENDPOINT = https://graph.facebook.com/me SUPER_USER_NAME = admin SUPER_USER_PASSWORD = admin SUPER_USER_EMAIL = admin@example.com SUPER_USER_BIRTHDATE = 1970-01-01 DATABASE_TYPE = postgresql POSTGRES_USER = admin POSTGRES_PASSWORD = admin POSTGRES_DB = app POSTGRES_SERVER = db # do not change this -> a change here requires a docker-compose change as well REDIS_URI = redis://redis:6379 # do not change this -> a change here requires a docker-compose change as well Gunicorn Environment Variables \u00b6 To maintain consistency, we've re-written gunicorn config file in gunicorn_config.py and used pydantic BaseSettings class to read environment variables for gunicorn. As a result, you can pass those variables by creating a file called gunicorn.env , check an example below: WORKERS_PER_CORE = 1 .0 HOST = 0 .0.0.0 PORT = 80 LOG_LEVEL = INFO GRACEFUL_TIMEOUT = 120 TIMEOUT = 120 KEEP_ALIVE = 5 Handling the Database \u00b6 ALEMBIC is now being used to handle database migrations, so please make sure to use it instead of manually modifying the database. After changing some database model, run alembic revision --autogenerate -m \"comment the model change here\" to create a new version file. Make sure you commit this file to the repo. Alembic migrations are run by the scripts/prestart.sh script when initializing the application via docker. In case there is any doubt, go to ALEMBIC Tutorial and check the instructions. Using HTTPS Locally \u00b6 It's always great to have our development environment as close as the production environment for testing purposes. So, developing with HTTPS locally is encouraged. Traefik makes it very easy to do that, even when using localhost. Here is a step by step on how to create your own certificates. This step by step was pretty much copied from Freecodecamp . Step 1 - Generate a Root SSL Certificate \u00b6 Generate a RSA-2048 key and save it to a file rootCA.key. This file will be used as the key to generate the Root SSL certificate. You will be prompted for a pass phrase which you\u2019ll need to enter each time you use this particular key to generate a certificate. openssl genrsa -des3 -out rootCA.key 2048 openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem Step 2 - Add the certificate to trusted certificate on your machine \u00b6 You might need to search on how to do it on your machine, I'll briefly explain it here for Windows OS. Search for certificates on Windows and open the Certificates Manager. Open up the folder where it says Trusted Root Certificates, or something similar to it. Open up the certificates folder. Right click on the right window, go to \"All Tasks\" --> Import. Finally, import the rootCA.pem we've just created. If this is not showing up, just select \"Show all files\" and select the *.pem file. Step 3 - Domain SSL certificate \u00b6 The root SSL certificate can now be used to issue a certificate specifically for your local development environment located at localhost. Create a new OpenSSL configuration file server.csr.cnf so you can import these settings when creating a certificate instead of entering them on the command line. [req] default_bits = 2048 prompt = no default_md = sha256 distinguished_name = dn [dn] C = US ST = RandomState L = RandomCity O = RandomOrganization OU = RandomOrganizationUnit emailAddress = hello@example.com CN = localhost Create a v3.ext file in order to create a X509 v3 certificate. Notice how we\u2019re specifying subjectAltName here. authorityKeyIdentifier = keyid,issuer basicConstraints = CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = localhost Create a certificate key for localhost using the configuration settings stored in server.csr.cnf. This key is stored in server.key. openssl req -new -sha256 -nodes -out server.csr -newkey rsa:2048 -keyout server.key -config <( cat server.csr.cnf ) A certificate signing request is issued via the root SSL certificate we created earlier to create a domain certificate for localhost. The output is a certificate file called server.crt. openssl x509 -req -in server.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out server.crt -days 500 -sha256 -extfile v3.ext Step 4 - Import them to traefik \u00b6 Finally, we can use our generated certificate and key (server.key and server.crt) to encrypt our HTTP connection using Traefik. On the docker-compose file under the Traefik service, there is a list of volumes defined: volumes: - ... - ./cert/server:/etc/certs:ro - ... - ... Make sure the certificate server.crt and the key server.key are referenced to the container on /etc/certs:ro . On the example above both the certificate and the key are stored under /cert/server on my local machine. You can store them on this path as well to make your life easier. Using docker-compose \u00b6 To run this application, all you need to do is run the following command: docker-compose up -d --build This will start the following services: Traefik to serve as a reverse proxy for our application Redis to serve as a cache service for our application Postgres to serve as a database for our application ELK -> there are two services using logstash, one serves as an agent that receives logs from the docker GELF driver and send them to Redis, the other one acts as a consumer that fetches those logs from Redis and send them to elasticsearch. Finally, we use Kibana to visualize our logs. This very api will also be started by docker under the service name of web Hopefully , if everything worked as expected, you will be able to access the application on Caishen User API","title":"Deployment"},{"location":"deployment/#deployment","text":"Set an .env file Make sure to setup the .env file because this will be necessary to start the application using docker Development environment I highly suggest you use linux to deploy this app, if you are on windows make sure you are developing under WSL2","title":"Deployment"},{"location":"deployment/#environment-variables","text":"Before running the app, make sure to create a .env file on project root. All environment variables available are declared on on settings.py . As this project uses pydantic BaseSettings, the .env file will populate the Settings class accordingly. Below is a sample of how your .env file shall look.","title":"Environment Variables"},{"location":"deployment/#caishen-enviroment-variables","text":"DEV = False LOG_LEVEL = DEBUG GOOGLE_CLIENT_ID = #replace: google-client-id-here GOOGLE_CLIENT_SECRET = #replace: google-client-secret-here GOOGLE_REDIRECT_URL = http://localhost:8000/api/v1/login/google-login-callback/ GOOGLE_DISCOVERY_URL = https://accounts.google.com/.well-known/openid-configuration FACEBOOK_CLIENT_ID = #replace: facebook-app-id-here FACEBOOK_CLIENT_SECRET = #replace: facebook-app-secret-here FACEBOOK_REDIRECT_URL = https://localhost/api/v1/login/facebook-login-callback FACEBOOK_DISCOVERY_URL = https://www.facebook.com/.well-known/openid-configuration/ FACEBOOK_AUTHORIZATION_ENDPOINT = https://facebook.com/dialog/oauth/ FACEBOOK_TOKEN_ENDPOINT = https://graph.facebook.com/oauth/access_token FACEBOOK_USERINFO_ENDPOINT = https://graph.facebook.com/me SUPER_USER_NAME = admin SUPER_USER_PASSWORD = admin SUPER_USER_EMAIL = admin@example.com SUPER_USER_BIRTHDATE = 1970-01-01 DATABASE_TYPE = postgresql POSTGRES_USER = admin POSTGRES_PASSWORD = admin POSTGRES_DB = app POSTGRES_SERVER = db # do not change this -> a change here requires a docker-compose change as well REDIS_URI = redis://redis:6379 # do not change this -> a change here requires a docker-compose change as well","title":"Caishen Enviroment Variables"},{"location":"deployment/#gunicorn-environment-variables","text":"To maintain consistency, we've re-written gunicorn config file in gunicorn_config.py and used pydantic BaseSettings class to read environment variables for gunicorn. As a result, you can pass those variables by creating a file called gunicorn.env , check an example below: WORKERS_PER_CORE = 1 .0 HOST = 0 .0.0.0 PORT = 80 LOG_LEVEL = INFO GRACEFUL_TIMEOUT = 120 TIMEOUT = 120 KEEP_ALIVE = 5","title":"Gunicorn Environment Variables"},{"location":"deployment/#handling-the-database","text":"ALEMBIC is now being used to handle database migrations, so please make sure to use it instead of manually modifying the database. After changing some database model, run alembic revision --autogenerate -m \"comment the model change here\" to create a new version file. Make sure you commit this file to the repo. Alembic migrations are run by the scripts/prestart.sh script when initializing the application via docker. In case there is any doubt, go to ALEMBIC Tutorial and check the instructions.","title":"Handling the Database"},{"location":"deployment/#using-https-locally","text":"It's always great to have our development environment as close as the production environment for testing purposes. So, developing with HTTPS locally is encouraged. Traefik makes it very easy to do that, even when using localhost. Here is a step by step on how to create your own certificates. This step by step was pretty much copied from Freecodecamp .","title":"Using HTTPS Locally"},{"location":"deployment/#step-1-generate-a-root-ssl-certificate","text":"Generate a RSA-2048 key and save it to a file rootCA.key. This file will be used as the key to generate the Root SSL certificate. You will be prompted for a pass phrase which you\u2019ll need to enter each time you use this particular key to generate a certificate. openssl genrsa -des3 -out rootCA.key 2048 openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.pem","title":"Step 1 - Generate a Root SSL Certificate"},{"location":"deployment/#step-2-add-the-certificate-to-trusted-certificate-on-your-machine","text":"You might need to search on how to do it on your machine, I'll briefly explain it here for Windows OS. Search for certificates on Windows and open the Certificates Manager. Open up the folder where it says Trusted Root Certificates, or something similar to it. Open up the certificates folder. Right click on the right window, go to \"All Tasks\" --> Import. Finally, import the rootCA.pem we've just created. If this is not showing up, just select \"Show all files\" and select the *.pem file.","title":"Step 2 - Add the certificate to trusted certificate on your machine"},{"location":"deployment/#step-3-domain-ssl-certificate","text":"The root SSL certificate can now be used to issue a certificate specifically for your local development environment located at localhost. Create a new OpenSSL configuration file server.csr.cnf so you can import these settings when creating a certificate instead of entering them on the command line. [req] default_bits = 2048 prompt = no default_md = sha256 distinguished_name = dn [dn] C = US ST = RandomState L = RandomCity O = RandomOrganization OU = RandomOrganizationUnit emailAddress = hello@example.com CN = localhost Create a v3.ext file in order to create a X509 v3 certificate. Notice how we\u2019re specifying subjectAltName here. authorityKeyIdentifier = keyid,issuer basicConstraints = CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = localhost Create a certificate key for localhost using the configuration settings stored in server.csr.cnf. This key is stored in server.key. openssl req -new -sha256 -nodes -out server.csr -newkey rsa:2048 -keyout server.key -config <( cat server.csr.cnf ) A certificate signing request is issued via the root SSL certificate we created earlier to create a domain certificate for localhost. The output is a certificate file called server.crt. openssl x509 -req -in server.csr -CA rootCA.pem -CAkey rootCA.key -CAcreateserial -out server.crt -days 500 -sha256 -extfile v3.ext","title":"Step 3 - Domain SSL certificate"},{"location":"deployment/#step-4-import-them-to-traefik","text":"Finally, we can use our generated certificate and key (server.key and server.crt) to encrypt our HTTP connection using Traefik. On the docker-compose file under the Traefik service, there is a list of volumes defined: volumes: - ... - ./cert/server:/etc/certs:ro - ... - ... Make sure the certificate server.crt and the key server.key are referenced to the container on /etc/certs:ro . On the example above both the certificate and the key are stored under /cert/server on my local machine. You can store them on this path as well to make your life easier.","title":"Step 4 - Import them to traefik"},{"location":"deployment/#using-docker-compose","text":"To run this application, all you need to do is run the following command: docker-compose up -d --build This will start the following services: Traefik to serve as a reverse proxy for our application Redis to serve as a cache service for our application Postgres to serve as a database for our application ELK -> there are two services using logstash, one serves as an agent that receives logs from the docker GELF driver and send them to Redis, the other one acts as a consumer that fetches those logs from Redis and send them to elasticsearch. Finally, we use Kibana to visualize our logs. This very api will also be started by docker under the service name of web Hopefully , if everything worked as expected, you will be able to access the application on Caishen User API","title":"Using docker-compose"},{"location":"project_structure/alembic/","text":"Alembic \u00b6 Coming soon.","title":"Alembic"},{"location":"project_structure/alembic/#alembic","text":"Coming soon.","title":"Alembic"},{"location":"project_structure/core/","text":"Core \u00b6 Coming soon.","title":"Core"},{"location":"project_structure/core/#core","text":"Coming soon.","title":"Core"},{"location":"project_structure/crud/","text":"Create Update Read Delete \u00b6 Coming soon.","title":"CRUD"},{"location":"project_structure/crud/#create-update-read-delete","text":"Coming soon.","title":"Create Update Read Delete"},{"location":"project_structure/dependencies/","text":"Dependencies \u00b6 Coming soon.","title":"Dependencies"},{"location":"project_structure/dependencies/#dependencies","text":"Coming soon.","title":"Dependencies"},{"location":"project_structure/models/","text":"Models \u00b6 Coming soon.","title":"Models"},{"location":"project_structure/models/#models","text":"Coming soon.","title":"Models"},{"location":"project_structure/overview/","text":"Project Structure Overview \u00b6 This is how the project is organized, directory wise. Project folders \u00b6 . \u251c\u2500\u2500 api \u251c\u2500\u2500 cert \u251c\u2500\u2500 docs \u251c\u2500\u2500 logstash \u251c\u2500\u2500 scripts \u2514\u2500\u2500 traefik The project code is located under the api folder. While the documentation is under the docs folder. The cert folder contains local ssl certificates that help us develop locally with HTTPS enabled. The traefik folder contains Traefik configuration files. The logstash folder contains Logstash configuration files. The scripts folder contains scripts that are used to deploy our application. Understand the api folder organization \u00b6 . \u251c\u2500\u2500 alembic \u251c\u2500\u2500 core \u251c\u2500\u2500 crud \u251c\u2500\u2500 dependencies \u251c\u2500\u2500 models \u251c\u2500\u2500 routers \u251c\u2500\u2500 schemas \u2514\u2500\u2500 utils This project structure is greatly inspired by this basic FastAPI Project Generator . Let's go ahead and check what each folder contains. Alembic \u00b6 The alembic folder contains alembic related files, this folder was created using the alembic init command. . \u251c\u2500\u2500 alembic \u2502 \u251c\u2500\u2500 README \u2502 \u251c\u2500\u2500 env.py \u2502 \u251c\u2500\u2500 script.py.mako \u2502 \u2514\u2500\u2500 versions We had to make some adjustments to env.py to adapt it to our project settings, these will be later explained on Alembic own article. Core \u00b6 The core folder contains, as you might guess, core functionality of our application. . \u251c\u2500\u2500 core \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 exceptions \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 authentication.py \u2502 \u2502 \u251c\u2500\u2500 authorization.py \u2502 \u2502 \u251c\u2500\u2500 base.py \u2502 \u2502 \u251c\u2500\u2500 cache.py \u2502 \u2502 \u251c\u2500\u2500 database.py \u2502 \u2502 \u2514\u2500\u2500 handlers.py \u2502 \u251c\u2500\u2500 logging \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 caishen_logger.py \u2502 \u2502 \u2514\u2500\u2500 settings.py \u2502 \u251c\u2500\u2500 security.py \u2502 \u2514\u2500\u2500 serializers.py \u2502 \u2514\u2500\u2500 database.py It holds configuration about our application exceptions, logging, security and serializers. The exceptions folder contains not only exception class definitions, but also handlers that are later used in the main.py file as FastAPI exceptions handlers. The logging folder has our default logger defined under caishen_logger.py but also all logging definition defined under settings.py which uses pydantic BaseSettings class to do some magic . The security model defines only password hashing functionality, finally the serializers are there just so we can wrap orjson dumps method, so we can return a string instead of bytes as recommended on pydantic documentation . The database model defines the database Session which we use to connect and interact with the database. CRUD \u00b6 CRUD stands for Create Read Update Delete, which are operations used in REST APIs. Those operations usually requires database connections. It means that every database operation for each database we've configured are defined under this folder. \u251c\u2500\u2500 crud \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 base.py \u2502 \u2514\u2500\u2500 user.py Currently we have only one database table defined: users , so apart from the base CRUD from which every other CRUD inherits, only user CRUD is defined. Dependencies \u00b6 Dependencies is where we define our FastAPI dependency injection. FastAPI Dependency Injection \"Dependency Injection\" means, in programming, that there is a way for your code (in this case, your path operation functions) to declare things that it requires to work and use: \"dependencies\". And then, that system (in this case FastAPI) will take care of doing whatever is needed to provide your code with those needed dependencies (\"inject\" the dependencies). This is very useful when you need to: Have shared logic (the same code logic again and again). Share database connections. Enforce security, authentication, role requirements, etc. And many other things... Reference: FastAPI Dependencies Tutorial \u251c\u2500\u2500 dependencies \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 auth \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 providers.py \u2502 \u2502 \u251c\u2500\u2500 schemes.py \u2502 \u2502 \u2514\u2500\u2500 security.py \u2502 \u251c\u2500\u2500 cache.py \u2502 \u251c\u2500\u2500 database.py \u2502 \u2514\u2500\u2500 user.py In our case we have a few dependency injections needed. cache for getting a Redis connection. database for getting a PostgreSQL connection. user for getting the current user who is accessing our application. auth where we define all the magic needed to login with OAuth providers like Google and Facebook and also to login with our Local provider (which is not Oauth). Auth dependencies This auth folder was previously under core , however as the majority of its classes and functions are used as dependencies I decided to move it to the dependencies folder instead. This is a great example of arbitrary choices I've made in order to organize this project. You might have different ideas about it and implement it in a different structure, and that's okay. Also, notice that things might change overtime, so don't get stuck because you don't know exactly where a specific piece of code should go. Models \u00b6 This is where all SQLAlchemy model classes are defined. \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 base.py \u2502 \u2514\u2500\u2500 user.py For now, only the user model is defined, which inherits from base . Schemas \u00b6 This is where all Pydantic model classes are defined. \u251c\u2500\u2500 schemas \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 auth.py \u2502 \u251c\u2500\u2500 base.py \u2502 \u251c\u2500\u2500 logs.py \u2502 \u251c\u2500\u2500 token.py \u2502 \u2514\u2500\u2500 user.py Pydantic is used to (de)serialize data to and from our API. The user file for example, has several user schemas defined. One for each different use cases we might have on our application. We have a UserRead, UserCreate, UserCreateLocal, and others. There is also schemas for logs, auth and tokens. Those are better explained on their own articles. Routers \u00b6 We don't want to specify every endpoint on a single file like main.py . To better organize our project we make use of the APIRouter class from FastAPI which allows us to define endpoints (aka routes) in different modules and later import them to our application using the include_router method. \u251c\u2500\u2500 routers \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 login.py \u2502 \u2514\u2500\u2500 users.py We have two defined routers so far, one for user operations and another one for login operations. Utils \u00b6 The utils folder is where we place code that we have no idea where else to place. \u2514\u2500\u2500 utils \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 check_logstash_status.py \u2514\u2500\u2500 init_db.py Seriously, those are just helper functions, for example, we have a function to check when the logstash agent we use to ship logs to Redis is ready, so we can delay our application startup until them. And the second function is to initialize our database with superuser so we can have admin privilege in our application before we start it.","title":"Overview"},{"location":"project_structure/overview/#project-structure-overview","text":"This is how the project is organized, directory wise.","title":"Project Structure Overview"},{"location":"project_structure/overview/#project-folders","text":". \u251c\u2500\u2500 api \u251c\u2500\u2500 cert \u251c\u2500\u2500 docs \u251c\u2500\u2500 logstash \u251c\u2500\u2500 scripts \u2514\u2500\u2500 traefik The project code is located under the api folder. While the documentation is under the docs folder. The cert folder contains local ssl certificates that help us develop locally with HTTPS enabled. The traefik folder contains Traefik configuration files. The logstash folder contains Logstash configuration files. The scripts folder contains scripts that are used to deploy our application.","title":"Project folders"},{"location":"project_structure/overview/#understand-the-api-folder-organization","text":". \u251c\u2500\u2500 alembic \u251c\u2500\u2500 core \u251c\u2500\u2500 crud \u251c\u2500\u2500 dependencies \u251c\u2500\u2500 models \u251c\u2500\u2500 routers \u251c\u2500\u2500 schemas \u2514\u2500\u2500 utils This project structure is greatly inspired by this basic FastAPI Project Generator . Let's go ahead and check what each folder contains.","title":"Understand the api folder organization"},{"location":"project_structure/overview/#alembic","text":"The alembic folder contains alembic related files, this folder was created using the alembic init command. . \u251c\u2500\u2500 alembic \u2502 \u251c\u2500\u2500 README \u2502 \u251c\u2500\u2500 env.py \u2502 \u251c\u2500\u2500 script.py.mako \u2502 \u2514\u2500\u2500 versions We had to make some adjustments to env.py to adapt it to our project settings, these will be later explained on Alembic own article.","title":"Alembic"},{"location":"project_structure/overview/#core","text":"The core folder contains, as you might guess, core functionality of our application. . \u251c\u2500\u2500 core \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 exceptions \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 authentication.py \u2502 \u2502 \u251c\u2500\u2500 authorization.py \u2502 \u2502 \u251c\u2500\u2500 base.py \u2502 \u2502 \u251c\u2500\u2500 cache.py \u2502 \u2502 \u251c\u2500\u2500 database.py \u2502 \u2502 \u2514\u2500\u2500 handlers.py \u2502 \u251c\u2500\u2500 logging \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 caishen_logger.py \u2502 \u2502 \u2514\u2500\u2500 settings.py \u2502 \u251c\u2500\u2500 security.py \u2502 \u2514\u2500\u2500 serializers.py \u2502 \u2514\u2500\u2500 database.py It holds configuration about our application exceptions, logging, security and serializers. The exceptions folder contains not only exception class definitions, but also handlers that are later used in the main.py file as FastAPI exceptions handlers. The logging folder has our default logger defined under caishen_logger.py but also all logging definition defined under settings.py which uses pydantic BaseSettings class to do some magic . The security model defines only password hashing functionality, finally the serializers are there just so we can wrap orjson dumps method, so we can return a string instead of bytes as recommended on pydantic documentation . The database model defines the database Session which we use to connect and interact with the database.","title":"Core"},{"location":"project_structure/overview/#crud","text":"CRUD stands for Create Read Update Delete, which are operations used in REST APIs. Those operations usually requires database connections. It means that every database operation for each database we've configured are defined under this folder. \u251c\u2500\u2500 crud \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 base.py \u2502 \u2514\u2500\u2500 user.py Currently we have only one database table defined: users , so apart from the base CRUD from which every other CRUD inherits, only user CRUD is defined.","title":"CRUD"},{"location":"project_structure/overview/#dependencies","text":"Dependencies is where we define our FastAPI dependency injection. FastAPI Dependency Injection \"Dependency Injection\" means, in programming, that there is a way for your code (in this case, your path operation functions) to declare things that it requires to work and use: \"dependencies\". And then, that system (in this case FastAPI) will take care of doing whatever is needed to provide your code with those needed dependencies (\"inject\" the dependencies). This is very useful when you need to: Have shared logic (the same code logic again and again). Share database connections. Enforce security, authentication, role requirements, etc. And many other things... Reference: FastAPI Dependencies Tutorial \u251c\u2500\u2500 dependencies \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 auth \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u251c\u2500\u2500 providers.py \u2502 \u2502 \u251c\u2500\u2500 schemes.py \u2502 \u2502 \u2514\u2500\u2500 security.py \u2502 \u251c\u2500\u2500 cache.py \u2502 \u251c\u2500\u2500 database.py \u2502 \u2514\u2500\u2500 user.py In our case we have a few dependency injections needed. cache for getting a Redis connection. database for getting a PostgreSQL connection. user for getting the current user who is accessing our application. auth where we define all the magic needed to login with OAuth providers like Google and Facebook and also to login with our Local provider (which is not Oauth). Auth dependencies This auth folder was previously under core , however as the majority of its classes and functions are used as dependencies I decided to move it to the dependencies folder instead. This is a great example of arbitrary choices I've made in order to organize this project. You might have different ideas about it and implement it in a different structure, and that's okay. Also, notice that things might change overtime, so don't get stuck because you don't know exactly where a specific piece of code should go.","title":"Dependencies"},{"location":"project_structure/overview/#models","text":"This is where all SQLAlchemy model classes are defined. \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 base.py \u2502 \u2514\u2500\u2500 user.py For now, only the user model is defined, which inherits from base .","title":"Models"},{"location":"project_structure/overview/#schemas","text":"This is where all Pydantic model classes are defined. \u251c\u2500\u2500 schemas \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 auth.py \u2502 \u251c\u2500\u2500 base.py \u2502 \u251c\u2500\u2500 logs.py \u2502 \u251c\u2500\u2500 token.py \u2502 \u2514\u2500\u2500 user.py Pydantic is used to (de)serialize data to and from our API. The user file for example, has several user schemas defined. One for each different use cases we might have on our application. We have a UserRead, UserCreate, UserCreateLocal, and others. There is also schemas for logs, auth and tokens. Those are better explained on their own articles.","title":"Schemas"},{"location":"project_structure/overview/#routers","text":"We don't want to specify every endpoint on a single file like main.py . To better organize our project we make use of the APIRouter class from FastAPI which allows us to define endpoints (aka routes) in different modules and later import them to our application using the include_router method. \u251c\u2500\u2500 routers \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 login.py \u2502 \u2514\u2500\u2500 users.py We have two defined routers so far, one for user operations and another one for login operations.","title":"Routers"},{"location":"project_structure/overview/#utils","text":"The utils folder is where we place code that we have no idea where else to place. \u2514\u2500\u2500 utils \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 check_logstash_status.py \u2514\u2500\u2500 init_db.py Seriously, those are just helper functions, for example, we have a function to check when the logstash agent we use to ship logs to Redis is ready, so we can delay our application startup until them. And the second function is to initialize our database with superuser so we can have admin privilege in our application before we start it.","title":"Utils"},{"location":"project_structure/routers/","text":"Routers \u00b6 Coming soon.","title":"Routers"},{"location":"project_structure/routers/#routers","text":"Coming soon.","title":"Routers"},{"location":"project_structure/schemas/","text":"Schemas \u00b6 Coming soon.","title":"Schemas"},{"location":"project_structure/schemas/#schemas","text":"Coming soon.","title":"Schemas"},{"location":"project_structure/utils/","text":"Utils \u00b6 Coming soon.","title":"Utils"},{"location":"project_structure/utils/#utils","text":"Coming soon.","title":"Utils"}]}